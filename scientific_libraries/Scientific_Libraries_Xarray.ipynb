{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adeb708",
   "metadata": {},
   "source": [
    "## Working with Xarray\n",
    "Xarry is a class of objects added to the regular python system that allows storing data in a more organized method. The format is very similar to netCDF classic model (netCDF3). It can read netCDF files efficiently and handle some issues associated with incorrectly designed netCDF files.\n",
    "\n",
    "Xarray also has extetions to use the Numpy, Pandas, Dask and SciPy libaries directly. Think of Xarray as a tool for orgnaizing data in a way that other libaries can be used on the data efficiently.\n",
    "\n",
    "The primary difference between Xarray and Pandas is that Pandas is designed to handle 1-D data while Xarray can handle n-D data and metadata about the data.\n",
    "\n",
    "The one downside is that Xarry has very powerful functions with less great documentation. May need to dig a bit to get the best way to perform a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc25c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr  # Convention is to import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72847190",
   "metadata": {},
   "source": [
    "## DataArray\n",
    "Here we create some data with Numpy and put into an Xarray DataArray. Notice how there is a concept of dimentionality built into DataArray. \"xarray.DataArray  (dim_0: 10000)\". But because we didn't define the dimention name a generic one was created for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66466a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(10000)  # This is a numpy array\n",
    "xr_da = xr.DataArray(data)  # Create the Xarray DataArray using Numpy array.\n",
    "xr_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697097f",
   "metadata": {},
   "source": [
    "This time create a time array to match the data array shape. Time will be one minute time steps. The time array will become a coordinate variable to describe the values along the dimention we defined and named \"time\". The coordinate is set to the time array and the dimention is set to \"time\" string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44706a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.array('2019-11-01T00:00:00', dtype='datetime64[m]') + np.arange(data.size)\n",
    "xr_da = xr.DataArray(data, dims=['time'], coords=[time])\n",
    "xr_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84afa51",
   "metadata": {},
   "source": [
    "We can add attributes describing metadata about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaba86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_da.attrs['long_name'] = 'Amazing data that will win me a Nobel prize.'\n",
    "xr_da.attrs['units'] = 'degK'\n",
    "xr_da.attrs['valid_min'] = 0.\n",
    "xr_da.attrs['valid_max'] = 10000.\n",
    "xr_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3d04c",
   "metadata": {},
   "source": [
    "Same as above but all in one step while creating the DataArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b84fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_da = xr.DataArray(\n",
    "    data, dims=['time'],\n",
    "    coords=[time],\n",
    "    attrs={'long_name': 'Amazing data that will win me a Nobel prize.',\n",
    "           'units': 'degK',\n",
    "           'valid_min': 0.,\n",
    "           'valid_max': 10000.})\n",
    "xr_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c31f50",
   "metadata": {},
   "source": [
    "To extract the data values only we use the .values attribute on the DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38586aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_da.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a37e61",
   "metadata": {},
   "source": [
    "To extract the attributes as a dictionary we use the .attrs attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff2f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_da.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7906b",
   "metadata": {},
   "source": [
    "Or the attrs decorator can also accept a name for a specific attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_da.attrs['long_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(xr_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28648022",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(xr_da.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f72aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(xr_da.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1691c4c5",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The full power of Xarray comes from using Datasets. A Dataset is a collection of DataArrays. The beauty of Datasets is holding all the corresoponding data together and performing functions on multiple DataArrays in the Datasets all at once. This becomes very powerful and very fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f08c49",
   "metadata": {},
   "source": [
    "Create some data and a time data array to match the data we created with minute time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e475540",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.arange(10000, dtype=float)\n",
    "data2 = np.arange(10000, dtype=float) + 123.456\n",
    "time = np.array('2019-11-01T00:00:00', dtype='datetime64[m]') + np.arange(data1.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6257f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_ds = xr.Dataset(\n",
    "    # This is the data section.\n",
    "    # Notice all data is wrappted in a dictionary. In that dictionary the key\n",
    "    # is the variable name followed by a tuple. The first value of the tuple\n",
    "    # is the dimension(s) name, folloed by the data values, followed by optional\n",
    "    # dictionary of attributes as key:value pairs.\n",
    "    data_vars={'data1': ('time', data1, {'long_name': 'Data 1 values', 'units': 'degC'}),\n",
    "               'data2': ('time', data2, {'long_name': 'Data 2 values', 'units': 'degF'})\n",
    "               },\n",
    "    # This is the coordinate section following the same format. Since this\n",
    "    # comes next it could be interpredted as positional as coordinates.\n",
    "    # But we are using keywords to make it easier to understand.\n",
    "    coords={'time': ('time', time, {'long_name': 'Time in UTC'})},\n",
    "    # Lastly we have the global attributes.\n",
    "    attrs={'the_best_animals': 'sharks'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7f362",
   "metadata": {},
   "source": [
    "Print out the full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c09e9b",
   "metadata": {},
   "source": [
    "Print out one DataArray from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315853e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_ds['data1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c58bd3",
   "metadata": {},
   "source": [
    "Print out values from the one variable in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_ds['data1'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fea406",
   "metadata": {},
   "source": [
    "Print out one attribute from one DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_ds['data1'].attrs['units']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb94fb",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "Let's read in a single netCDF data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe488ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "filename = Path('..', 'data', 'sgpmetE13.b1', 'sgpmetE13.b1.20191101.000000.cdf')\n",
    "met_ds = xr.open_dataset(filename)\n",
    "met_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3152de",
   "metadata": {},
   "source": [
    "We can also read in multiple netCDF data files using a differnet method. All the kewords accecpted by open_dataset() are accepted by open_mfdataset()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98add8ba",
   "metadata": {},
   "source": [
    "The filename glob is understood by open_mfdataset() and correctly grabs all the files that match the file glob. Using parallel=True allows it to use multiple cores for reading the data. This may depend on your machine and number of cores available, and may be faster or may not. We can also reduce the amount of memory required by excluding some variables from being read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93acd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = str(Path('..', 'data', 'sgpmetE13.b1', 'sgpmetE13.b1.*.cdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523ea95",
   "metadata": {},
   "source": [
    "To resolve issues with incorrectly formatted variables or reduce the memory we can exclude some variable from being read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe7a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_vars = [\n",
    "    'base_time', 'time_offset', 'vapor_pressure_std', 'wspd_arith_mean',\n",
    "    'qc_wspd_arith_mean', 'wspd_vec_mean', 'qc_wspd_vec_mean',\n",
    "    'wdir_vec_mean', 'qc_wdir_vec_mean', 'wdir_vec_std', 'tbrg_precip_total',\n",
    "    'qc_tbrg_precip_total', 'tbrg_precip_total_corr', 'qc_tbrg_precip_total_corr',\n",
    "    'org_precip_rate_mean', 'qc_org_precip_rate_mean', 'pwd_err_code',\n",
    "    'pwd_mean_vis_1min', 'qc_pwd_mean_vis_1min', 'pwd_mean_vis_10min',\n",
    "    'qc_pwd_mean_vis_10min', 'pwd_pw_code_inst', 'qc_pwd_pw_code_inst',\n",
    "    'pwd_pw_code_15min', 'qc_pwd_pw_code_15min', 'pwd_pw_code_1hr',\n",
    "    'qc_pwd_pw_code_1hr', 'pwd_precip_rate_mean_1min',\n",
    "    'qc_pwd_precip_rate_mean_1min', 'pwd_cumul_rain', 'qc_pwd_cumul_rain',\n",
    "    'pwd_cumul_snow', 'qc_pwd_cumul_snow', 'logger_volt', 'qc_logger_volt',\n",
    "    'logger_temp', 'qc_logger_temp', 'temp_std', 'rh_std', 'vapor_pressure_mean',\n",
    "    'qc_vapor_pressure_mean', 'a_very_long_name_that_is_not_in_the_data_file']\n",
    "\n",
    "\n",
    "met_ds = xr.open_mfdataset(filename, drop_variables=drop_vars,\n",
    "                           parallel=True)\n",
    "\n",
    "met_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c1e27",
   "metadata": {},
   "source": [
    "Once we have the data read, Xarray has a wrapper around matplotlib to generate plots directly from the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "met_ds['temp_mean'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b537a3",
   "metadata": {},
   "source": [
    "Here we make two plots. Syntax is slightly different than calling matplotlib directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362dee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2)\n",
    "met_ds['temp_mean'].plot(ax=axes[0])\n",
    "met_ds['rh_mean'].plot(ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0162ef",
   "metadata": {},
   "source": [
    "## Xarray playing very well with other libraries\n",
    "We can use Xarray with Pandas natively for even easier actions.\n",
    "Here we use pandas to make a time array with a 6 hour time step for four years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_time = pd.date_range('2000-01-01', freq='6H', periods=365 * 4)\n",
    "pd_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f35a8",
   "metadata": {},
   "source": [
    "We will create a new Xarray Dataset with a range of numbers matching the number of time samples we created. The Pandas time is used to initialize the xarray Dataset. Because Pandas and Xarray play well together it just works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265627d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_ds = xr.Dataset({'data': ('time', np.arange(len(pd_time))), 'time': pd_time})\n",
    "xr_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c35fb1",
   "metadata": {},
   "source": [
    "## Sub-selecting data\n",
    "Now let's calculate the mean for a day by grouping the data. This works for all time worded groups: hour, minute, year, month, ... The method returns a new Dataset and leaves the orginal untouched. Notice how _data_ was orginanally type integer, but because we are calcualting a mean, the type is upconverted to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0581a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mean = xr_ds.groupby('time.day').mean()\n",
    "ds_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a76624",
   "metadata": {},
   "source": [
    "We can also define the grouping size by using the .resample() method and pass in the funciton to use using the .reduce() method. If there is no data to perform the operation, a new time step is added with a NaN data value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4f535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_mean = xr_ds.resample(time='30min').reduce(np.nanmean)\n",
    "ds_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4618ee1",
   "metadata": {},
   "source": [
    "The .sel() method select data based on data or coordinate values. We can extract a range of data by filtering on the time coordinate and using the builtin slice() function. This looks familar to the Pandas example because Xarray is using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d14771",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset = xr_ds.sel(time=slice('2000-06-01 06:00', '2000-08-03 23:59:59'))\n",
    "ds_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc711eae",
   "metadata": {},
   "source": [
    "For selecting at specific index use .isel()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset = xr_ds.isel({'time': range(200, 832)})\n",
    "ds_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02d8b3",
   "metadata": {},
   "source": [
    "What if we want to find the closes value in time but not match to values outside a tolerable range. We can use .reindex() method with a toleracne to indicate which values should be matched. The values that don't have a match are set to NaN. We need to use timedelta64() to set the tolerance value which includes a time unit. Anywhere the time is outside the tolerance the data values is set to NaN. To allow for setting NaN, the data type is upconverted to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace72825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show how the tolerance work add some random seconds to the time used in matching\n",
    "subset_time = xr_ds['time'].values \n",
    "random_seconds = np.random.randint(-10, 10, size=subset_time.size).astype('timedelta64[s]')\n",
    "subset_time = subset_time + random_seconds\n",
    "\n",
    "ds_result = xr_ds.reindex(time=subset_time, method='nearest',\n",
    "                          tolerance=np.timedelta64(5, 's'))\n",
    "print(xr_ds['data'].values[:10])\n",
    "print(ds_result['data'].values[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47551a7a",
   "metadata": {},
   "source": [
    "To not have a Dataset full of missing values where the time was outside the tolerance we can drop where all values are set to NaN and return a new Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b42a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of time values:', ds_result.dims['time'])\n",
    "ds_result = ds_result.dropna('time', how='all')\n",
    "print('Number of time values:', ds_result.dims['time'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
