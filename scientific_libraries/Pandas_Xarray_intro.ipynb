{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7100b380-4297-4077-9d0d-32ea8d26ef55",
   "metadata": {},
   "source": [
    "# Starting with Scientific Libraries in Python\n",
    "\n",
    "If you want to perform some larger scientific computations in Python some great libraries to use are:\n",
    "- Numpy = base building blocks for large array based computing\n",
    "- Pandas = Pandas is an extension of the Numpy data model to structure and organize data into Dataframes. A Dataframe resembles the look of a spreadsheet, but is much more powerful. \n",
    "- Xarray = Xarray is an extension of the Numpy and Pandas data model to structure and organize data into Datasets. A Dataset resembles the look of a netCDF3 data model, but is much more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af252d-0953-4551-a405-93a3c7115a6b",
   "metadata": {},
   "source": [
    "## Working with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eabe56-d968-4784-b040-e7f5bdb79221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Convention suggests to import as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf45adda-a1fd-45c0-926f-3d6e3ab94ac9",
   "metadata": {},
   "source": [
    "## Create Pandas Dataframe\n",
    "Create some data in a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5b4c0-466b-43a9-86ab-1e4688394630",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'apples': [3, 2, 0, 1],\n",
    "        'oranges': [0, 3, 7, 2]}\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f7875c-c97a-42f8-bad1-9b83a36a68eb",
   "metadata": {},
   "source": [
    "Use the dictionary to populate the new Pandas data frame. Python and Jupyter play very well with Pandas and when requested to print the Pandas Dataframe or Series Jupyter knows how to print in a visually appealing way. Notice the columns have names taken from the Dictionary keys we used to populate the Dataframe. Each row has an index set to an incremented integer by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec6e908-3059-46af-a595-b5fa33333e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases = pd.DataFrame(data)\n",
    "purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6eec6-7eeb-43db-b450-b447a24590b7",
   "metadata": {},
   "source": [
    "A Dataframe has a concept of index to label the rows of the Dataframe and column names to label the Columns. We created column names from the dictionary keys used to initially create the Dataframe. We can create index labels in the Dataframe upon creation of the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38b1a3-81b3-41e2-ade7-522576c325d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases = pd.DataFrame(data, index=['June', 'Robert', 'Lily', 'David'])\n",
    "purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc19e7db-3d8b-4142-b41d-fe050f53cbe5",
   "metadata": {},
   "source": [
    "Printing the column shows additional information including the index and data type. This is returned as a Pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86118cdc-a72b-423b-beb5-4eebe8a1499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases.apples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19d1d6-1099-4abf-998c-5f92644776b1",
   "metadata": {},
   "source": [
    "Extracting a row uses the .loc() method on the Dataframe. .loc() standard for location and searches the index for a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ea5e3-3847-46d8-991e-9d2aa008c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases.loc['June']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2b7c2-8eb5-485d-92cd-a5410a8f8779",
   "metadata": {},
   "source": [
    "Can also extract using index location .iloc() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa724762-af89-4b8c-be0d-a8da9ce6252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe022ef-55db-4fb2-bf7e-dc27bd625414",
   "metadata": {},
   "source": [
    "## Read in some data\n",
    "Using the path to a specific file, we read the data into a _Pandas_ _Dataframe_. This is something you will use very often if you have data in ASCII column files. Get to know this method well. The method has keywords to help describe how to read the column data including the delimiter, number of header rows, and which column is the time stamp. If it can parse the time stamp it will convert to Pandas native time type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c5d61-5407-48b7-a25a-42917be126e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # Importing a sub-module of the pathlib library\n",
    "import numpy as np  # Convention is to import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce8893-84a4-4317-826f-827dec673ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path('..', 'data', 'sgpmetE13.b1', 'sgpmetE13.00.20191105.150801.raw.dat')\n",
    "df = pd.read_csv(filename, delimiter=',', skiprows=[0, 2, 3], header=0, parse_dates=[0])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41778e7e-c6d0-425f-9dc8-0e1e5ad3f560",
   "metadata": {},
   "source": [
    "Similar to Numpy, can use the .shape and .size methods on the Dataframe to return metadata about size of the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eaca5a-dc22-45e1-89f1-4bf6798687bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.size)\n",
    "print(df.shape)  # 60 rows by 33 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82467f32-f1fd-4116-9eb0-a33f48cb3d3f",
   "metadata": {},
   "source": [
    "Print the first five rows using Numpy syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0099f41-aaa8-43e8-b24d-370e9fbc9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:5]  # Prints from row 0 to 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e74f4-8f07-438d-8eae-4eaa54d64954",
   "metadata": {},
   "source": [
    "We can get the names of the columns with .columns method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d061a-cb98-49ac-8ddf-5f37a281587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79017ca-99e3-44c5-86ce-0fa12d7f48f8",
   "metadata": {},
   "source": [
    "Pandas has a few methods used to inspect the data in the Dataset or Series and present the typical statistical values. Because Pandas plays so well with Jupyter, the output is very easy to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2a887-6b40-4788-a288-0be3c35974fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f95d1a-574b-44f1-af53-9a8ddc31297b",
   "metadata": {},
   "source": [
    "What type is the TIMESTAMP data? Notice that Pandas has a new data type of time specific to Pandas. This is similar to Datetime datetime and Numpy datetime64, but is technically different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb953d-24ff-42ca-a03c-ed14968c7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df['TIMESTAMP'][0]))\n",
    "print(type(df['PTemp'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1cbeb-4593-4364-a357-d10aca2512af",
   "metadata": {},
   "source": [
    "Get the pressure Series from the DataFrame and sum it up to one value and calculate the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd519f-e165-4908-9752-0fffaf05b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Pressure_kPa'].sum())\n",
    "print(df['Pressure_kPa'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b82a721-b225-432e-b5ba-084e9b6e7d6a",
   "metadata": {},
   "source": [
    "Extract the RH series from the DataFrame. This is a copy of the Series in the DataFrame, so changing the values will not change the values in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce05e6-6b87-40b4-b730-ba9be99b782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rh = df['RH_Avg']\n",
    "print(type(rh))\n",
    "rh_np = np.array(rh)\n",
    "print(type(rh_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ff597-2000-4502-aa1c-7b19ec75970b",
   "metadata": {},
   "source": [
    "Calculate a rolling mean over the Series using 10 points. Notice the first value is NaN. There is a default number of values to use to calculate a value. Else it is set to NaN. By specifically stating the minimum number of points to use when calculating the mean, we force it to not fill in so many NaNs. There are at least two values to use in the rolling window, so only first value is set to NaN. What happens when you change min_periods to a larger number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73074cf-5b7f-4041-81ff-fda4bb7408f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_rolling_mean = df['RH_Avg'].rolling(10, min_periods=2).mean()\n",
    "rh_rolling_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0282a-3657-4ad8-99ce-d12f5c67d63c",
   "metadata": {},
   "source": [
    "# Working with Xarray\n",
    "Xarray is a class of objects added to the regular Python system that allows storing data in a more organized method. The format is very similar to the netCDF classic model (netCDF3). It can read netCDF files efficiently and handle some issues associated with incorrectly designed netCDF files.\n",
    "\n",
    "Xarray also has extentions to use the Numpy, Pandas, Dask and SciPy libaries directly. Think of Xarray as a tool for organizing data in a way that other libraries can be used on the data efficiently.\n",
    "\n",
    "A primary difference between Xarray and Pandas is that Pandas is designed to handle 1-D data while Xarray can handle n-D data and metadata about the data.\n",
    "\n",
    "The one downside is that Xarray has very powerful functions with less great documentation. May need to dig a bit to get the best way to perform a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7212e7-2b94-43cd-ba3b-12c7bceb7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr  # Convention is to import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3d6e7-9636-4cba-857e-e14d5f145575",
   "metadata": {},
   "source": [
    "## DataArray\n",
    "Here we create some data with Numpy and put into an Xarray DataArray. Notice how there is a concept of dimensionality built into DataArray. \"xarray.DataArray  (dim_0: 10000)\". But because we didn't define the dimension name, a generic one was created for us. Also notice how nice the printing looks. Xarray plays very well with Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852b2b3-bf6c-4afe-9e8a-40f61b9d3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(10000)  # This is a numpy array\n",
    "da = xr.DataArray(data)  # Create the Xarray DataArray using Numpy array.\n",
    "da  # Convention suggests using the variable da for DataArray when feasable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6715a5-0173-4975-be0f-9526f2359656",
   "metadata": {},
   "source": [
    "This time create a time array to match the data array shape. Time will be one minute time steps. The time array will become a coordinate variable to describe the values along the dimension we defined and named \"time\". The coordinate is set to the time array and the dimension is set to \"time\" string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee351b8d-b275-4417-bbbb-d816fd89e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.array('2019-11-01T00:00:00', dtype='datetime64[m]') + np.arange(data.size)\n",
    "time = time.astype('datetime64[ns]')  # Only done to stop a warning appearing\n",
    "da = xr.DataArray(data, dims=['time'], coords=[time])\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8a84a-5d58-4d90-b600-e5b700e9fb4b",
   "metadata": {},
   "source": [
    "We can add attributes describing metadata about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502ccd5-da32-46b4-8a0f-7f2730817085",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.attrs['long_name'] = 'Amazing data that will win me a Nobel prize.'\n",
    "da.attrs['units'] = 'degK'\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063685c4-6098-49ae-ab22-e7c0c22670f6",
   "metadata": {},
   "source": [
    "Same as above, but an all in one step while creating the DataArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433cae91-f7d6-448c-a4e6-052682208f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.DataArray(\n",
    "    data, dims=['time'],\n",
    "    coords=[time],\n",
    "    attrs={'long_name': 'Amazing data that will win me a Nobel prize.',\n",
    "           'units': 'degK'})\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3246645c-7369-4c6d-8fcd-cd94a6b87d9a",
   "metadata": {},
   "source": [
    "To extract the data values only, we use the .values attribute on the DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5573718d-7d7f-4829-b3c3-5a26e7a7f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeeaa4b-1297-4292-aed5-e93b18458cad",
   "metadata": {},
   "source": [
    "To extract the attributes as a dictionary, we use the .attrs attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e0775-977c-4d1d-a2c3-d1ec021852b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a74bd4-b9b8-42c0-a4f0-4bea5ef8e3a3",
   "metadata": {},
   "source": [
    "Or the attrs decorator can also accept a name for a specific attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ec2c5-7cb3-44ae-96b5-9e7352ba3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.attrs['long_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee482726-90e0-4f09-843a-b5e396cc6dea",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The full power of Xarray comes from using Datasets. A Dataset is a collection of DataArrays. The beauty of Datasets is holding all the corresoponding data together and performing functions on multiple DataArrays in the Datasets all at once. This becomes very powerful and extremely fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c86eb-5a14-4f40-88a7-7884322a3413",
   "metadata": {},
   "source": [
    "Create some data and a time data array to match the data we created with minute time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5519470-0720-4566-bcc2-97a52a7d8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.arange(10000, dtype=float)\n",
    "data2 = np.arange(10000, dtype=float) + 123.456\n",
    "time = np.array('2019-11-01T00:00:00', dtype='datetime64[m]') + np.arange(data1.size)\n",
    "time = time.astype('datetime64[ns]')  # Only done to stop a warning appearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab516724-0008-4995-9b98-bc158e423c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset(\n",
    "    # This is the data section.\n",
    "    # Notice all data is wrapped in a dictionary. In that dictionary the key\n",
    "    # is the variable name followed by a tuple. The first value of the tuple\n",
    "    # is the dimension(s) name, followed by the data values, followed by an optional\n",
    "    # dictionary of attributes as key:value pairs.\n",
    "    data_vars={'data1': ('time', data1, {'long_name': 'Data 1 values', 'units': 'degC'}),\n",
    "               'data2': ('time', data2, {'long_name': 'Data 2 values', 'units': 'degF'})\n",
    "               },\n",
    "    # This is the coordinate section following the same format. Since this\n",
    "    # comes next it could be interpreted as positional as coordinates.\n",
    "    # But we are using keywords to make it easier to understand.\n",
    "    coords={'time': ('time', time, {'long_name': 'Time in UTC'})},\n",
    "    # Lastly, we have the global attributes.\n",
    "    attrs={'the_best_animals': 'sharks'}\n",
    ")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db3a62c-7067-44b9-b461-2e263a730a3b",
   "metadata": {},
   "source": [
    "Print out one DataArray from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682a7dc0-78ab-4c44-8546-04637ef791b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['data1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7217c332-f55a-4189-a486-39ff53b63bcc",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "Let's read in a single netCDF data file. Notice we are using relative paths to go one directory down into the _data_ directory. This will also read in multiple files when provided a list of file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72156e7-d067-48b9-8115-ce70ba28a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path('..', 'data', 'sgpmetE13.b1', 'sgpmetE13.b1.20191101.000000.cdf')\n",
    "ds = xr.open_mfdataset(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd27b4d-ba31-493c-aedb-f87c656b58b5",
   "metadata": {},
   "source": [
    "To resolve issues with incorrectly formatted variables or reduce the memory, we can exclude some variables from being read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a922c-6d52-4f60-9291-14913989a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_vars = [\n",
    "    'base_time', 'time_offset', 'vapor_pressure_std', 'wspd_arith_mean',\n",
    "    'qc_wspd_arith_mean', 'wspd_vec_mean', 'qc_wspd_vec_mean',\n",
    "    'wdir_vec_mean', 'qc_wdir_vec_mean', 'wdir_vec_std', 'tbrg_precip_total',\n",
    "    'qc_tbrg_precip_total', 'tbrg_precip_total_corr', 'qc_tbrg_precip_total_corr',\n",
    "    'org_precip_rate_mean', 'qc_org_precip_rate_mean', 'pwd_err_code',\n",
    "    'pwd_mean_vis_1min', 'qc_pwd_mean_vis_1min', 'pwd_mean_vis_10min',\n",
    "    'qc_pwd_mean_vis_10min', 'pwd_pw_code_inst', 'qc_pwd_pw_code_inst',\n",
    "    'pwd_pw_code_15min', 'qc_pwd_pw_code_15min', 'pwd_pw_code_1hr',\n",
    "    'qc_pwd_pw_code_1hr', 'pwd_precip_rate_mean_1min',\n",
    "    'qc_pwd_precip_rate_mean_1min', 'pwd_cumul_rain', 'qc_pwd_cumul_rain',\n",
    "    'pwd_cumul_snow', 'qc_pwd_cumul_snow', 'logger_volt', 'qc_logger_volt',\n",
    "    'logger_temp', 'qc_logger_temp', 'temp_std', 'rh_std', 'vapor_pressure_mean',\n",
    "    'qc_vapor_pressure_mean', 'a_very_long_name_that_is_not_in_the_data_file',\n",
    "    'qc_atmos_pressure', 'qc_temp_mean', 'qc_rh_mean']\n",
    "\n",
    "\n",
    "ds = xr.open_mfdataset(filename, drop_variables=drop_vars)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816043c0-1404-40ed-9237-b7012736f34a",
   "metadata": {},
   "source": [
    "We can perform the same operations of sum and mean on the DataArray as we did with the Pandas Series. Notice the returned value is a Numpy array of length one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882412b-18e9-49be-8316-6724a8d7de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds['atmos_pressure'].sum().values)\n",
    "print(ds['atmos_pressure'].mean().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9226bb29-2c9b-4eb5-8738-bcc9d23ab4c0",
   "metadata": {},
   "source": [
    "## Sub-selecting data\n",
    "Now let's calculate the mean for an hour by grouping the data. This works for all time worded groups: day, hour, minute, year, month, ... The method returns a new Dataset and leaves the original untouched. Notice how _data_ was originally type integer, but because we are calculating a mean, the type is upconverted to a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966eeb81-c6cb-424f-92a4-562140a363fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = ds.groupby('time.hour').mean()\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51cc4e-9dc1-4c81-8372-6dedb860a3c1",
   "metadata": {},
   "source": [
    "We can also define the grouping size by using the .resample() method and pass in the function to use using the .reduce() method. If there is no data to perform the operation, a new time step is added with a NaN data value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710dcd0-4c12-4fa0-b881-d83d85cb627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = ds.resample(time='30min').reduce(np.nanmean)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3950cc-a956-416a-9abb-01a752abd63d",
   "metadata": {},
   "source": [
    "The .sel() method select data based on data or coordinate values. We can extract a range of data by filtering on the time coordinate and using the builtin slice() function. This looks familar to the Pandas example because Xarray is using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90665151-3327-4a59-bb5a-caf0f99c6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = ds.sel(time=slice('2019-11-01 06:00', '2019-11-01 20:59:59'))\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e9f63-6c88-41c5-ad9c-e25fe423192f",
   "metadata": {},
   "source": [
    "For selecting at specific index use .isel()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6d37b-e7cb-4900-bd7f-a85c5fe2ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = ds.isel({'time': range(200, 832)})\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d59c9f-b084-423a-9c9d-5a8c1f8909fe",
   "metadata": {},
   "source": [
    "What if we want to find the closest value in time but not match to values outside a tolerable range? We can use the .reindex() method with a tolerance to indicate which values should be matched. The values that don't have a match are set to NaN. We need to use timedelta64() to set the tolerance value which includes a time unit. Anywhere the time is outside the tolerance, the data values are set to NaN. To allow for setting NaN, the data type is upconverted to a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56db6173-c318-4cf6-a750-6e45ec244eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show how the tolerance works, add some random seconds to the time used in matching\n",
    "subset_time = ds['time'].values \n",
    "random_seconds = np.random.randint(-10, 10, size=subset_time.size).astype('timedelta64[s]')\n",
    "subset_time = subset_time + random_seconds\n",
    "\n",
    "ds_result = ds.reindex(time=subset_time, method='nearest',\n",
    "                       tolerance=np.timedelta64(5, 's'))\n",
    "print(ds['atmos_pressure'].values[:10])\n",
    "print(ds_result['atmos_pressure'].values[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
